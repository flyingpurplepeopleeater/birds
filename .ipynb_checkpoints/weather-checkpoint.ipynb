{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may need to follow the instructions at the link below to actually open the big tar file:\n",
    "# https://stackoverflow.com/questions/43288550/iopub-data-rate-exceeded-in-jupyter-notebook-when-viewing-image\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import tarfile\n",
    "from io import StringIO\n",
    "import re\n",
    "import math\n",
    "from IPython import display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This is just so that the progress information while the notebook is running is prettier\n",
    "class printer(str):\n",
    "    def __repr__(self):\n",
    "       return self\n",
    "\n",
    "class idisplay:\n",
    "    def __init__(self,str):\n",
    "        self.myobject = display.display(printer(str), display_id=True)\n",
    "        \n",
    "    def update(self,str):\n",
    "        return self.myobject.update(printer(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This allows us to read the ghcnd data\n",
    "# copy-pasted with np fixes and index removal for metadata\n",
    "# https://gitlab.com/-/snippets/1838910\n",
    "\n",
    "metadata_col_specs = [\n",
    "    (0,  12),\n",
    "    (12, 21),\n",
    "    (21, 31),\n",
    "    (31, 38),\n",
    "    (38, 41),\n",
    "    (41, 72),\n",
    "    (72, 76),\n",
    "    (76, 80),\n",
    "    (80, 86)\n",
    "]\n",
    "\n",
    "metadata_names = [\n",
    "    \"ID\",\n",
    "    \"LATITUDE\",\n",
    "    \"LONGITUDE\",\n",
    "    \"ELEVATION\",\n",
    "    \"STATE\",\n",
    "    \"NAME\",\n",
    "    \"GSN FLAG\",\n",
    "    \"HCN/CRN FLAG\",\n",
    "    \"WMO ID\"]\n",
    "\n",
    "metadata_dtype = {\n",
    "    \"ID\": str,\n",
    "    \"STATE\": str,\n",
    "    \"NAME\": str,\n",
    "    \"GSN FLAG\": str,\n",
    "    \"HCN/CRN FLAG\": str,\n",
    "    \"WMO ID\": str\n",
    "    }\n",
    "\n",
    "\n",
    "# Data specs #\n",
    "\n",
    "data_header_names = [\n",
    "    \"ID\",\n",
    "    \"YEAR\",\n",
    "    \"MONTH\",\n",
    "    \"ELEMENT\"]\n",
    "\n",
    "data_header_col_specs = [\n",
    "    (0,  11),\n",
    "    (11, 15),\n",
    "    (15, 17),\n",
    "    (17, 21)]\n",
    "\n",
    "data_header_dtypes = {\n",
    "    \"ID\": str,\n",
    "    \"YEAR\": int,\n",
    "    \"MONTH\": int,\n",
    "    \"ELEMENT\": str}\n",
    "\n",
    "data_col_names = [[\n",
    "    \"VALUE\" + str(i + 1),\n",
    "    \"MFLAG\" + str(i + 1),\n",
    "    \"QFLAG\" + str(i + 1),\n",
    "    \"SFLAG\" + str(i + 1)]\n",
    "    for i in range(31)]\n",
    "# Join sub-lists\n",
    "data_col_names = sum(data_col_names, [])\n",
    "\n",
    "data_replacement_col_names = [[\n",
    "    (\"VALUE\", i + 1),\n",
    "    (\"MFLAG\", i + 1),\n",
    "    (\"QFLAG\", i + 1),\n",
    "    (\"SFLAG\", i + 1)]\n",
    "    for i in range(31)]\n",
    "# Join sub-lists\n",
    "data_replacement_col_names = sum(data_replacement_col_names, [])\n",
    "data_replacement_col_names = pd.MultiIndex.from_tuples(\n",
    "    data_replacement_col_names,\n",
    "    names=['VAR_TYPE', 'DAY'])\n",
    "\n",
    "data_col_specs = [[\n",
    "    (21 + i * 8, 26 + i * 8),\n",
    "    (26 + i * 8, 27 + i * 8),\n",
    "    (27 + i * 8, 28 + i * 8),\n",
    "    (28 + i * 8, 29 + i * 8)]\n",
    "    for i in range(31)]\n",
    "data_col_specs = sum(data_col_specs, [])\n",
    "\n",
    "data_col_dtypes = [{\n",
    "    \"VALUE\" + str(i + 1): int,\n",
    "    \"MFLAG\" + str(i + 1): str,\n",
    "    \"QFLAG\" + str(i + 1): str,\n",
    "    \"SFLAG\" + str(i + 1): str}\n",
    "    for i in range(31)]\n",
    "data_header_dtypes.update({k: v for d in data_col_dtypes for k, v in d.items()})\n",
    "\n",
    "\n",
    "# Reading functions #\n",
    "\n",
    "def read_station_metadata(filename=\"ghcnd-stations.txt\"):\n",
    "    \"\"\"Reads in station metadata\n",
    "\n",
    "    :filename: ghcnd station metadata file.\n",
    "    :returns: station metadata as a pandas Dataframe\n",
    "\n",
    "    \"\"\"\n",
    "    df = pd.read_fwf(filename, colspecs = metadata_col_specs, names=metadata_names, dtype=metadata_dtype)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_ghcn_data_file(filename=\"ghcnd_all/ACW00011604.dly\",\n",
    "                        variables=None, include_flags=False,\n",
    "                        dropna='all'):\n",
    "    \"\"\"Reads in all data from a GHCN .dly data file\n",
    "\n",
    "    :param filename: path to file\n",
    "    :param variables: list of variables to include in output dataframe\n",
    "        e.g. ['TMAX', 'TMIN', 'PRCP']\n",
    "    :param include_flags: Whether to include data quality flags in the final output\n",
    "    :returns: Pandas dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_fwf(\n",
    "        filename,\n",
    "        colspecs=data_header_col_specs + data_col_specs,\n",
    "        names=data_header_names + data_col_names,\n",
    "        index_col=data_header_names,\n",
    "        dtype=data_header_dtypes\n",
    "        )\n",
    "\n",
    "    if variables is not None:\n",
    "        df = df[df.index.get_level_values('ELEMENT').isin(variables)]\n",
    "\n",
    "    df.columns = data_replacement_col_names\n",
    "\n",
    "    if not include_flags:\n",
    "        df = df.loc[:, ('VALUE', slice(None))]\n",
    "        df.columns = df.columns.droplevel('VAR_TYPE')\n",
    "\n",
    "    df = df.stack(level='DAY').unstack(level='ELEMENT').reset_index(level=(\"ID\",))\n",
    "\n",
    "    if dropna:\n",
    "        df.replace(-9999.0, np.nan, inplace=True)\n",
    "        df.dropna(how=dropna, inplace=True)\n",
    "\n",
    "    # replace the entire index with the date.\n",
    "    # This loses the station ID index column!\n",
    "    # This will usuall fail if dropna=False, since months with <31 days\n",
    "    # still have day=31 columns\n",
    "    # df['DATE'] = pd.to_datetime(\n",
    "    #     df.index.get_level_values('YEAR') * 10000 +\n",
    "    #     df.index.get_level_values('MONTH') * 100 +\n",
    "    #     df.index.get_level_values('DAY'),\n",
    "    #     format='%Y%m%d')\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening up our tar file\n",
    "\n",
    "tar = tarfile.open(\"data/ghcnd_all.tar.gz\",\"r:gz\")\n",
    "allmembers = tar.getmembers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For this project, we only care about the region defined by the lat-long restrictions\n",
    "# in the first 4 lines of this cell. This finds the stations in that region; this doesn't need to be run\n",
    "# if the Data/valid_stations.txt file already exists with the desired regional parameters!\n",
    "\n",
    "lat_min = 28.0\n",
    "lat_max = 49.0\n",
    "long_min = -97.0\n",
    "long_max = -68.0\n",
    "\n",
    "valid_stations=[]\n",
    "station_data = read_station_metadata(\"Data/ghcnd-stations.txt\")\n",
    "console = idisplay(\"Starting...\")\n",
    "valid_count = 0\n",
    "i = 1\n",
    "while i<len(allmembers)-1:\n",
    "    addend = read_ghcn_data_file(StringIO(tar.extractfile(allmembers[i]).read().decode('utf-8')))\n",
    "    station = re.search(\"(?<=\\/)(.+)(?=\\.)\",allmembers[i].name).group(0)\n",
    "    my_station_data = station_data.loc[station_data[\"ID\"]==station].iloc[0]\n",
    "    lat = math.floor(1.0*my_station_data[[\"LATITUDE\"]].iloc[0])/1.0\n",
    "    long = math.floor(1.0*my_station_data[[\"LONGITUDE\"]].iloc[0])/1.0\n",
    "    if (not (lat<lat_min or lat >=lat_max or long < long_min or long >= long_max)):\n",
    "        valid_stations.append(i)\n",
    "        valid_count+=1\n",
    "    console.update(\"Scraped \"+str(i)+\", found \"+str(valid_count))\n",
    "    i+=1\n",
    "f = open(\"Data/valid_stations.txt\", \"a\")\n",
    "f.write(str(valid_stations))\n",
    "f.close()\n",
    "console.update(\"Scraped \"+str(i-1)+\", found \"+str(valid_count)+\"; done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile station data\n",
    "\n",
    "weather_by_grid = pd.DataFrame()\n",
    "station_data = read_station_metadata(\"Data/ghcnd-stations.txt\")\n",
    "valid_stations = list(map(int,list(open('Data/valid_stations.txt', 'r').read()[1:-1].split(\", \"))))\n",
    "get_count = -1\n",
    "\n",
    "console = idisplay(\"You have \"+str(len(valid_stations))+\" stations! You're about to scrape the first \"\n",
    "                   +str(min(len(allmembers)-1,get_count))+\".\")\n",
    "\n",
    "win_count = 0\n",
    "for i in range(0,len(valid_stations)):\n",
    "    addend = read_ghcn_data_file(StringIO(tar.extractfile(allmembers[valid_stations[i]]).read().decode('utf-8')))\n",
    "    station = re.search(\"(?<=\\/)(.+)(?=\\.)\",allmembers[valid_stations[i]].name).group(0)\n",
    "    my_station_data = station_data.loc[station_data[\"ID\"]==station].iloc[0]\n",
    "    lat = math.floor(1.0*my_station_data[[\"LATITUDE\"]].iloc[0])/1.0\n",
    "    long = math.floor(1.0*my_station_data[[\"LONGITUDE\"]].iloc[0])/1.0\n",
    "    addend[[\"LATITUDE\",\"LONGITUDE\"]]=[lat,long]\n",
    "    addend = addend.set_index([\"LATITUDE\",\"LONGITUDE\"],append=True)\n",
    "    addend = addend[addend.columns.intersection([\"PRCP\",\n",
    "                                                \"SNOW\",\n",
    "                                                \"TMAX\",\n",
    "                                                \"TMIN\"])]\n",
    "    for j in addend.columns:\n",
    "        addend[j+'_CNT']=addend[j].apply(lambda a : int((not isinstance(a,float)) or (not math.isnan(a))))\n",
    "    addend = addend.loc[addend.index.get_level_values(\"YEAR\")>2016]\n",
    "    if win_count == 0:\n",
    "        weather_by_grid = addend\n",
    "    else:\n",
    "        weather_by_grid = weather_by_grid.add(addend,fill_value = 0)\n",
    "    win_count+=1\n",
    "    console.update(str(valid_stations[i])+\" \"+str(win_count)+\" \"+str(weather_by_grid.size))\n",
    "    if win_count==get_count:\n",
    "        break\n",
    "console.update(\"You've scraped \"+str(win_count)+\" stations; done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See some of the data we've put into weather_by_grid!\n",
    "\n",
    "pd.set_option('display.min_rows',100)\n",
    "pd.set_option('display.max_rows',100)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=4,ncols=3,figsize=(10,10))\n",
    "\n",
    "for i in range(1,13):\n",
    "    wg = weather_by_grid\n",
    "    \n",
    "    # print(wg.index)\n",
    "    wg = wg[wg.index.isin([2019], level=\"YEAR\")]\n",
    "    wg = wg[wg.index.isin([i], level=\"MONTH\")]\n",
    "    wg=wg[wg.index.isin([7], level=\"DAY\")]\n",
    "    wg = wg.reset_index(level=\"LATITUDE\").reset_index(level=\"LONGITUDE\")\n",
    "    # print(wg[[\"TMAX\",\"TMAX_CNT\",\"LATITUDE\",\"LONGITUDE\"]])\n",
    "    # wg = wg.pivot(\"LATITUDE\",\"LONGITUDE\",\"TMAX\")\n",
    "    \n",
    "    # print(wg)\n",
    "    def average(row):\n",
    "        if row[\"TMAX_CNT\"]==0: return float(\"nan\")\n",
    "        else:\n",
    "            celsius = .1*(float(row[\"TMAX\"])/float(row[\"TMAX_CNT\"]))\n",
    "            return (celsius*9/5)+32\n",
    "            \n",
    "    wg[\"TMAX_AVG\"]=wg.apply(average, axis=1)\n",
    "    \n",
    "    wg = pd.pivot_table(wg,values='TMAX_AVG',\n",
    "                        index=\"LATITUDE\",\n",
    "                        columns=\"LONGITUDE\")\n",
    "    sns.heatmap(wg,ax=axes[math.floor((i-1)/3)][(i-1)%3],vmin=0,vmax=100).invert_yaxis()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export our data. Here, to weather_seven, named for the number of years of data included\n",
    "\n",
    "weather_by_grid.to_csv(r'data/weather_seven.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
